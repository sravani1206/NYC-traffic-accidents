{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc66fa4-5f34-4099-8226-c9b6195013fb",
     "showTitle": true,
     "title": "Azure Storage Account Configuration for Spark on Azure Databricks"
    }
   },
   "outputs": [],
   "source": [
    "# Define configurations for connecting to Azure Storage Account using Azure Databricks\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",  # Specify the authentication type\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",    \n",
    "    \"fs.azure.account.oauth2.client.id\": \"3896bfa0-a08a-47c3-ad54-2cc620c0f3e8\",    # Azure Active Directory (AAD) client ID\n",
    "    \"fs.azure.account.oauth2.client.secret\": 'qTu8Q~vhjC6JoiPtHITPa2lfuGnvejMOVu5PvbZP',    # AAD client secret\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/838623c7-bc8f-4f99-b81e-b8a1988042be/oauth2/token\"    # AAD token endpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08824122-a23c-4ae4-acb7-65d55eb64121",
     "showTitle": true,
     "title": "Mount External Storage in Databricks"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4236954440275105>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43msource\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mabfss://nyc-accidents-data@nycaccidentsdata.dfs.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# container@storageaccount\u001B[39;49;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43mmount_point\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/nycaccidents\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43mextra_configs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfigs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o772.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:601)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:977)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:750)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:966)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:609)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:386)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:386)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:332)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:402)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:402)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:377)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:157)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n",
       "\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:81)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:76)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-4236954440275105>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43msource\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mabfss://nyc-accidents-data@nycaccidentsdata.dfs.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# container@storageaccount\u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43mmount_point\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/nycaccidents\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43mextra_configs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfigs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o772.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:601)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:977)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:750)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:966)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:609)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:386)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:386)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:332)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:402)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:56)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:402)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:377)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:157)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:81)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:76)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nycaccidents; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount the external storage in Databricks\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://nyc-accidents-data@nycaccidentsdata.dfs.core.windows.net\", \n",
    "mount_point = \"/mnt/nycaccidents\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a36626-ea5f-4b8d-ba78-f0d422dac48d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/nycaccidents/raw-data/</td><td>raw-data/</td><td>0</td><td>1700920632000</td></tr><tr><td>dbfs:/mnt/nycaccidents/transformed-data/</td><td>transformed-data/</td><td>0</td><td>1700920648000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/nycaccidents/raw-data/",
         "raw-data/",
         0,
         1700920632000
        ],
        [
         "dbfs:/mnt/nycaccidents/transformed-data/",
         "transformed-data/",
         0,
         1700920648000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls \"mnt/nycaccidents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fdd7ee-5e83-49fa-988f-c23c8c84e14a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8970850511474535#setting/sparkui/1125-170952-ncuh9wd5/driver-3908262843264217074\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea470f3250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5893b0d1-46b6-45bc-bd2e-6e1729b357ff",
     "showTitle": true,
     "title": "Load Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Load the NYC dataset from a CSV file\n",
    "# - Format: \"csv\"\n",
    "# - Header: The first row contains column names\n",
    "# - Infer Schema: Automatically infer the data types of columns\n",
    "nycdata = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/nycaccidents/raw-data/nyc_accidents_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdfb89c-fe1b-4291-be55-ab910b364483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+----------+-------------------------+------------------------+-----------------------------+\n|CRASH DATE|         CRASH TIME| LATITUDE| LONGITUDE|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|\n+----------+-------------------+---------+----------+-------------------------+------------------------+-----------------------------+\n|2020-08-29|2023-11-29 15:40:00|  40.8921| -73.83376|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 21:00:00|  40.6905|-73.919914|                        2|                       0|                            0|\n|2020-08-29|2023-11-29 18:20:00|  40.8165|-73.946556|                        1|                       0|                            1|\n|2020-08-29|2023-11-29 00:00:00| 40.82472| -73.89296|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 17:10:00| 40.64989| -73.93389|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 03:29:00| 40.68231| -73.84495|                        1|                       0|                            0|\n|2020-08-29|2023-11-29 19:30:00|40.825226| -73.88778|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 00:00:00| 40.80016| -73.93538|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 19:50:00|40.894314| -73.86027|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 09:20:00| 40.70678| -73.90888|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 00:07:00|40.680237| -73.79774|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 14:00:00|40.704422|-73.792854|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 21:33:00|40.812965|  -73.9161|                        1|                       0|                            1|\n|2020-08-29|2023-11-29 22:53:00| 40.70166|-73.961464|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 04:14:00|40.835373|-73.842186|                        1|                       0|                            0|\n|2020-08-29|2023-11-29 06:35:00| 40.65965|-73.773834|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 13:00:00|40.699707| -73.95718|                        0|                       0|                            0|\n|2020-08-29|2023-11-29 10:30:00|  40.7122| -73.86208|                        2|                       0|                            0|\n|2020-08-29|2023-11-29 12:29:00|40.861862| -73.91282|                        2|                       0|                            0|\n|2020-08-29|2023-11-29 10:35:00|40.710957|-73.951126|                        1|                       0|                            0|\n+----------+-------------------+---------+----------+-------------------------+------------------------+-----------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "nycdata.select(\"CRASH DATE\", \"CRASH TIME\", \"LATITUDE\", \"LONGITUDE\", \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\", \"NUMBER OF PEDESTRIANS INJURED\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8df540a-d9ba-4662-a5c7-9f883a14eaa3",
     "showTitle": true,
     "title": "Print Schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CRASH DATE: date (nullable = true)\n |-- CRASH TIME: timestamp (nullable = true)\n |-- BOROUGH: string (nullable = true)\n |-- ZIP CODE: integer (nullable = true)\n |-- LATITUDE: double (nullable = true)\n |-- LONGITUDE: double (nullable = true)\n |-- LOCATION: string (nullable = true)\n |-- ON STREET NAME: string (nullable = true)\n |-- CROSS STREET NAME: string (nullable = true)\n |-- OFF STREET NAME: string (nullable = true)\n |-- NUMBER OF PERSONS INJURED: integer (nullable = true)\n |-- NUMBER OF PERSONS KILLED: integer (nullable = true)\n |-- NUMBER OF PEDESTRIANS INJURED: integer (nullable = true)\n |-- NUMBER OF PEDESTRIANS KILLED: integer (nullable = true)\n |-- NUMBER OF CYCLIST INJURED: integer (nullable = true)\n |-- NUMBER OF CYCLIST KILLED: integer (nullable = true)\n |-- NUMBER OF MOTORIST INJURED: integer (nullable = true)\n |-- NUMBER OF MOTORIST KILLED: integer (nullable = true)\n |-- CONTRIBUTING FACTOR VEHICLE 1: string (nullable = true)\n |-- CONTRIBUTING FACTOR VEHICLE 2: string (nullable = true)\n |-- CONTRIBUTING FACTOR VEHICLE 3: string (nullable = true)\n |-- CONTRIBUTING FACTOR VEHICLE 4: string (nullable = true)\n |-- CONTRIBUTING FACTOR VEHICLE 5: string (nullable = true)\n |-- COLLISION_ID: integer (nullable = true)\n |-- VEHICLE TYPE CODE 1: string (nullable = true)\n |-- VEHICLE TYPE CODE 2: string (nullable = true)\n |-- VEHICLE TYPE CODE 3: string (nullable = true)\n |-- VEHICLE TYPE CODE 4: string (nullable = true)\n |-- VEHICLE TYPE CODE 5: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "nycdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2030696-f2e5-4fa2-8c74-f19812315c40",
     "showTitle": true,
     "title": "NYC Traffic Collision Dataset Transformation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed NYC Dataset:\n+----------+-------------------+----+-----------+--------+---------+----------+-------------------------+------------------------+\n|CRASH DATE|         CRASH TIME|HOUR|DAY_OF_WEEK| BOROUGH| LATITUDE| LONGITUDE|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|\n+----------+-------------------+----+-----------+--------+---------+----------+-------------------------+------------------------+\n|2020-08-29|2023-11-29 15:40:00|  15|        Sat|   BRONX|  40.8921| -73.83376|                        0|                       0|\n|2020-08-29|2023-11-29 21:00:00|  21|        Sat|BROOKLYN|  40.6905|-73.919914|                        2|                       0|\n|2020-08-29|2023-11-29 18:20:00|  18|        Sat|    NULL|  40.8165|-73.946556|                        1|                       0|\n|2020-08-29|2023-11-29 00:00:00|   0|        Sat|   BRONX| 40.82472| -73.89296|                        0|                       0|\n|2020-08-29|2023-11-29 17:10:00|  17|        Sat|BROOKLYN| 40.64989| -73.93389|                        0|                       0|\n|2020-08-29|2023-11-29 00:00:00|   0|        Sat|    NULL| 40.80016| -73.93538|                        0|                       0|\n|2020-08-29|2023-11-29 19:50:00|  19|        Sat|   BRONX|40.894314| -73.86027|                        0|                       0|\n|2020-08-29|2023-11-29 00:07:00|   0|        Sat|  QUEENS|40.680237| -73.79774|                        0|                       0|\n|2020-08-29|2023-11-29 14:00:00|  14|        Sat|  QUEENS|40.704422|-73.792854|                        0|                       0|\n|2020-08-29|2023-11-29 13:00:00|  13|        Sat|BROOKLYN|40.699707| -73.95718|                        0|                       0|\n|2020-08-29|2023-11-29 10:30:00|  10|        Sat|  QUEENS|  40.7122| -73.86208|                        2|                       0|\n|2020-08-29|2023-11-29 12:29:00|  12|        Sat|   BRONX|40.861862| -73.91282|                        2|                       0|\n|2020-08-29|2023-11-29 10:35:00|  10|        Sat|BROOKLYN|40.710957|-73.951126|                        1|                       0|\n|2020-08-29|2023-11-29 13:55:00|  13|        Sat|BROOKLYN| 40.67473| -74.00029|                        1|                       0|\n|2020-08-29|2023-11-29 00:30:00|   0|        Sat|    NULL| 40.66584| -73.75551|                        0|                       0|\n|2020-08-29|2023-11-29 19:00:00|  19|        Sat|    NULL| 40.83968|-73.929276|                        1|                       0|\n|2020-08-29|2023-11-29 07:10:00|   7|        Sat|    NULL|40.674347| -73.82071|                        0|                       0|\n|2020-08-29|2023-11-29 00:56:00|   0|        Sat|   BRONX| 40.84307|-73.848076|                        1|                       0|\n|2020-08-29|2023-11-29 13:20:00|  13|        Sat|    NULL|40.680954| -73.96768|                        0|                       0|\n|2020-08-29|2023-11-29 22:11:00|  22|        Sat|BROOKLYN|40.608727| -73.95402|                        1|                       0|\n+----------+-------------------+----+-----------+--------+---------+----------+-------------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, hour\n",
    "\n",
    "# Transformation Steps\n",
    "\n",
    "# 1. Extract day of the week from the date\n",
    "df = nycdata.withColumn(\"DAY_OF_WEEK\", date_format(col(\"CRASH DATE\"), \"E\"))\n",
    "\n",
    "# 2. Extract hour from the timestamp\n",
    "df = df.withColumn(\"HOUR\", hour(col(\"CRASH TIME\")))\n",
    "\n",
    "# 3. Filter out rows with null values in relevant columns\n",
    "df = df.filter(col(\"CRASH DATE\").isNotNull() & col(\"CRASH TIME\").isNotNull())\n",
    "\n",
    "# 4. Filter out \"Unspecified\" contributing factors\n",
    "df = df.filter(col(\"CONTRIBUTING FACTOR VEHICLE 1\") != \"Unspecified\")\n",
    "\n",
    "# # 5. Select relevant columns for further analysis\n",
    "selected_columns = [\"CRASH DATE\", \"CRASH TIME\", \"HOUR\", \"DAY_OF_WEEK\", \"BOROUGH\", \"LATITUDE\", \"LONGITUDE\", \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\"]\n",
    "df_selected = df.select(*selected_columns)\n",
    "\n",
    "# Display the transformed dataset\n",
    "print(\"Transformed NYC Dataset:\")\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c18639-a544-4bdb-bc4d-4d566c8f918d",
     "showTitle": true,
     "title": "Top Accident Counts By Borough"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Boroughs with the Highest Accident Counts:\n+-------------+-----+\n|      BOROUGH|count|\n+-------------+-----+\n|     BROOKLYN|11240|\n|       QUEENS|10323|\n|        BRONX| 5907|\n|    MANHATTAN| 5657|\n|STATEN ISLAND| 1069|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out rows with null values in the BOROUGH column\n",
    "filtered_nycdata = df.filter(col(\"BOROUGH\").isNotNull())\n",
    "\n",
    "# Group by borough and count accidents\n",
    "borough_counts = filtered_nycdata.groupBy(\"BOROUGH\").count()\n",
    "\n",
    "# Get the top 5 boroughs with the highest accident counts\n",
    "top5_boroughs = borough_counts.orderBy(\"count\", ascending=False).limit(5)\n",
    "\n",
    "# Display the result\n",
    "print(\"Top 5 Boroughs with the Highest Accident Counts:\")\n",
    "top5_boroughs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa8203f-eade-42a4-8564-e9a5a6e20d5b",
     "showTitle": true,
     "title": "Analysis of Top Contributing Factors for Fatal Accidents"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Contributing Factors for Fatal Accidents:\n+-----------------------------+---------------+\n|CONTRIBUTING FACTOR VEHICLE 1|FatalitiesCount|\n+-----------------------------+---------------+\n|                 Unsafe Speed|             39|\n|         Traffic Control D...|             15|\n|         Driver Inattentio...|             13|\n|         Failure to Yield ...|             10|\n|         Pedestrian/Bicycl...|              6|\n|          Driver Inexperience|              6|\n|                       Illnes|              5|\n|             Backing Unsafely|              3|\n|         View Obstructed/L...|              3|\n|         Failure to Keep R...|              2|\n+-----------------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter fatal accidents\n",
    "fatal_accidents = df.filter(col(\"NUMBER OF PERSONS KILLED\") > 0)\n",
    "\n",
    "# Count fatalities by contributing factors\n",
    "factor_fatalities = fatal_accidents.groupBy(\"CONTRIBUTING FACTOR VEHICLE 1\").agg({\"NUMBER OF PERSONS KILLED\": \"sum\"}).withColumnRenamed(\"sum(NUMBER OF PERSONS KILLED)\", \"FatalitiesCount\")\n",
    "\n",
    "# Get the top 10 contributing factors with the highest fatal accident counts\n",
    "top10_factors_fatalities = factor_fatalities.orderBy(\"FatalitiesCount\", ascending=False).limit(10)\n",
    "\n",
    "# Display the result\n",
    "print(\"Top 10 Contributing Factors for Fatal Accidents:\")\n",
    "top10_factors_fatalities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca9ea22-97a4-4de0-bf52-0fc8dc765434",
     "showTitle": true,
     "title": "Analysis of Peak Accident Times"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Accident Times Sorted by Highest Count:\n+----+-----+\n|HOUR|count|\n+----+-----+\n|  16| 3857|\n|  14| 3706|\n|  17| 3648|\n|  18| 3469|\n|  15| 3399|\n|  13| 3265|\n|  12| 3009|\n|  11| 2834|\n|  19| 2754|\n|   8| 2677|\n+----+-----+\n\nPeak Accident Times Sorted by Least Count:\n+----+-----+\n|HOUR|count|\n+----+-----+\n|   3|  726|\n|   4|  727|\n|   2|  860|\n|   5|  889|\n|   1| 1117|\n|   6| 1416|\n|  23| 1639|\n|   7| 1808|\n|  22| 1874|\n|  21| 1978|\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, col\n",
    "\n",
    "# Extract hour from the timestamp\n",
    "df_peak_time = df.withColumn(\"HOUR\", hour(col(\"CRASH TIME\")))\n",
    "\n",
    "# Group by hour and count accidents\n",
    "peak_times = df_peak_time.groupBy(\"HOUR\").count()\n",
    "\n",
    "# Sort by the highest count for peak accident times\n",
    "sorted_peak_times = peak_times.orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Display only the top 10 peak accident times\n",
    "top_peak_times = sorted_peak_times.limit(10)\n",
    "\n",
    "# Display the result\n",
    "print(\"Peak Accident Times Sorted by Highest Count:\")\n",
    "top_peak_times.show()\n",
    "\n",
    "# Sort by the least count for peak accident times\n",
    "sorted_least_peak_times = peak_times.orderBy(\"count\", ascending=True)\n",
    "\n",
    "# Display only the top 10 peak accident times\n",
    "least_peak_times = sorted_least_peak_times.limit(10)\n",
    "\n",
    "# Display the result\n",
    "print(\"Peak Accident Times Sorted by Least Count:\")\n",
    "least_peak_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b563cb65-efd5-46dc-8d3d-b494d0b14523",
     "showTitle": true,
     "title": "Accident Distribution by Day of the Week"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day-wise Distribution of Accidents Sorted by Count:\n+-----------+-----+\n|DAY_OF_WEEK|count|\n+-----------+-----+\n|        Fri| 9044|\n|        Thu| 8251|\n|        Wed| 7924|\n|        Sat| 7803|\n|        Mon| 7755|\n|        Tue| 7709|\n|        Sun| 6620|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "# Extract day of the week from the date\n",
    "df_crash_date = df.withColumn(\"DAY_OF_WEEK\", date_format(col(\"CRASH DATE\"), \"E\"))\n",
    "\n",
    "# Group by day of the week and count accidents\n",
    "day_distribution = df_crash_date.groupBy(\"DAY_OF_WEEK\").count()\n",
    "\n",
    "# Sort by count in descending order\n",
    "sorted_day_distribution = day_distribution.orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(\"Day-wise Distribution of Accidents Sorted by Count:\")\n",
    "sorted_day_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f39d0b-ff24-4ee9-8f22-5f29ff88e5f4",
     "showTitle": true,
     "title": "Export Transformed Data"
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/mnt/nycaccidents/transformed-data/nyc_accidents_2020_transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a8d72d-22f8-4625-9c09-a2d8ce17f234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4236954440275106,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC Traffic Accidents Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
